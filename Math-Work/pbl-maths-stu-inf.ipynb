{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers datasets evaluate matplotlib seaborn nltk rouge-score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:44:02.547643Z","iopub.execute_input":"2025-04-16T19:44:02.547959Z","iopub.status.idle":"2025-04-16T19:44:08.947389Z","shell.execute_reply.started":"2025-04-16T19:44:02.547933Z","shell.execute_reply":"2025-04-16T19:44:08.946314Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=5e6aee688399765b6cd1dce4ef8c8b1d858a0cdade44ee95f8849b060e17c38b\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score, evaluate\nSuccessfully installed evaluate-0.4.3 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Enable CUDA debugging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:44:08.948494Z","iopub.execute_input":"2025-04-16T19:44:08.948726Z","iopub.status.idle":"2025-04-16T19:44:08.952635Z","shell.execute_reply.started":"2025-04-16T19:44:08.948706Z","shell.execute_reply":"2025-04-16T19:44:08.951762Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset\nfrom typing import Dict, List, Tuple\nimport time\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:44:08.954068Z","iopub.execute_input":"2025-04-16T19:44:08.954363Z","iopub.status.idle":"2025-04-16T19:44:18.131025Z","shell.execute_reply.started":"2025-04-16T19:44:08.954341Z","shell.execute_reply":"2025-04-16T19:44:18.130396Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import login\napi_token = \"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\"\nlogin(api_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:44:18.132164Z","iopub.execute_input":"2025-04-16T19:44:18.132689Z","iopub.status.idle":"2025-04-16T19:44:18.270189Z","shell.execute_reply.started":"2025-04-16T19:44:18.132656Z","shell.execute_reply":"2025-04-16T19:44:18.269391Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nimport pandas as pd\nimport re\nimport torch\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-medium\")\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-medium\").to(device)\n\nprint(\"Model loaded successfully in GPU!!!!!!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:44:43.458227Z","iopub.execute_input":"2025-04-16T19:44:43.458589Z","iopub.status.idle":"2025-04-16T19:45:03.854451Z","shell.execute_reply.started":"2025-04-16T19:44:43.458563Z","shell.execute_reply":"2025-04-16T19:45:03.853564Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"472b1b1b823041d4a6147fe1b8565810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce0b38ef6b36462785e9d75e750a499d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ed389ee0c7433f88bcf20cdd8824be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7df0b476e7ac4fb4a83dd683a87197b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42eb00fb598e430cb7f50826de7be39e"}},"metadata":{}},{"name":"stdout","text":"Model loaded successfully in GPU!!!!!!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import re\n# Load dataset and select first 100 entries\ndataset = load_dataset(\"qintongli/GSM-Plus\")\ndata = dataset['test'].select(range(100))\n\nresults = []\n\nfor example in data:\n\n    question = example['question']\n    prompt = f\"Answer this question with only the numerical result and no other text: {question}\\nAnswer:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    outputs = model.generate(\n        inputs.input_ids,\n        max_new_tokens=10,\n        num_beams=1,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id,\n        early_stopping=True\n    )\n    \n    full_answer = tokenizer.decode(outputs[0][inputs.input_ids.size(1):], skip_special_tokens=True)\n    \n    numerical_answer = re.search(r'[-+]?\\d*\\.?\\d+', full_answer)\n    clean_answer = numerical_answer.group() if numerical_answer else \"NA\"\n    \n    results.append({\n        'question': question,\n        'generated_answer': clean_answer,\n        'reference_answer': example['answer']\n    })\n\ndf = pd.DataFrame(results)\ndf.to_csv('/kaggle/working/gpt2_mid_gsmplus_inference.csv', index=False)\n\nprint(\"Inference complete. Results saved to gpt2_mid_gsmplus_inference.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:11:55.675811Z","iopub.execute_input":"2025-04-16T20:11:55.676143Z","iopub.status.idle":"2025-04-16T20:12:25.141489Z","shell.execute_reply.started":"2025-04-16T20:11:55.676114Z","shell.execute_reply":"2025-04-16T20:12:25.140735Z"}},"outputs":[{"name":"stdout","text":"Inference complete. Results saved to gpt2_mid_gsmplus_inference.csv.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\ndf = pd.read_csv(\"/kaggle/working/gpt2_mid_gsmplus_inference.csv\")  # Changed to match your output filename\n\ndef is_numeric(value):\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False\n\ndef normalize_number(num_str):\n    \"\"\"Remove trailing .0 from whole numbers and standardize format\"\"\"\n    if not is_numeric(num_str):\n        return num_str\n    num = float(num_str)\n    if num.is_integer():\n        return str(int(num))\n    return str(num)\n\ndef calculate_num_match(pred, ref):\n    \"\"\"Strict numerical comparison with normalization\"\"\"\n    try:\n        return int(float(pred) == float(ref))\n    except (ValueError, TypeError):\n        return 0\n\ndef calculate_num_close(pred, ref, rel_tol=1e-3):\n    \"\"\"Check if numbers are close (for floating point answers)\"\"\"\n    try:\n        pred_f = float(pred)\n        ref_f = float(ref)\n        return int(abs(pred_f - ref_f) <= rel_tol * abs(ref_f))\n    except (ValueError, TypeError):\n        return 0\n\nresults = []\nfor _, row in df.iterrows():\n    pred = str(row['generated_answer']).strip()\n    ref = str(row['reference_answer']).strip()\n    \n    results.append({\n        'exact_match': calculate_num_match(pred, ref),\n        'close_match': calculate_num_close(pred, ref),\n        'is_numeric': is_numeric(pred), \n        'pred_normalized': normalize_number(pred),\n        'ref_normalized': normalize_number(ref)\n    })\n\nmetrics_df = pd.DataFrame(results)\nfinal_df = pd.concat([df, metrics_df], axis=1)\n\nfinal_df.to_csv(\"/kaggle/working/gpt2_mid_gsmplus_evaluated.csv\", index=False)\n\n# Calculate aggregate metrics\naggregate_metrics = {\n    'Exact Match': np.mean(metrics_df['exact_match']),\n    'Close Match (±0.1%)': np.mean(metrics_df['close_match']),\n    'Valid Numerical Output': np.mean(metrics_df['is_numeric']),\n    'Accuracy': accuracy_score(\n        metrics_df['ref_normalized'],\n        metrics_df['pred_normalized']\n    )\n}\n\nprint(\"\\nNumerical Answer Evaluation Metrics:\")\nfor metric, value in aggregate_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\npd.DataFrame([aggregate_metrics]).to_csv(\"/kaggle/working/gpt2_mid_aggregate_metrics.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:19:29.280891Z","iopub.execute_input":"2025-04-16T20:19:29.281268Z","iopub.status.idle":"2025-04-16T20:19:29.315575Z","shell.execute_reply.started":"2025-04-16T20:19:29.281242Z","shell.execute_reply":"2025-04-16T20:19:29.314690Z"}},"outputs":[{"name":"stdout","text":"\nNumerical Answer Evaluation Metrics:\nExact Match: 0.0100\nClose Match (±0.1%): 0.0100\nValid Numerical Output: 1.0000\nAccuracy: 0.0300\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load dataset\n# dataset = load_dataset(\"qintongli/GSM-Plus\", split=\"test\")\n# subset = dataset.select(range(100))  # Take first 200 samples\n\n# # Prepare for inference\n# results = []\n\n# for i, sample in enumerate(subset):\n#     # Prepare input\n#     question = sample[\"question\"]\n#     input_ids = tokenizer.encode(question, return_tensors=\"pt\").to(device)\n    \n#     # Generate output\n#     with torch.no_grad():\n#         output = model.generate(\n#             input_ids,\n#             max_length=200,\n#             num_return_sequences=1,\n#             pad_token_id=tokenizer.eos_token_id\n#         )\n    \n#     # Decode output\n#     answer = tokenizer.decode(output[0], skip_special_tokens=True)\n    \n#     # Store results\n#     results.append({\n#         \"question\": question,\n#         \"reference_answer\": sample[\"answer\"],\n#         \"model_answer\": answer\n#     })\n    \n#     # Print progress\n#     if (i + 1) % 10 == 0:\n#         print(f\"Processed {i + 1} samples\")\n\n# # Save to CSV\n# df = pd.DataFrame(results)\n# df.to_csv(\"/kaggle/working/gpt2_mid_gsmplus_inference.csv\", index=False)\n# print(\"Inference results saved to gpt2_mid_gsmplus_inference.csv\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# from rouge_score import rouge_scorer\n# import numpy as np\n\n# # Load results\n# df = pd.read_csv(\"/kaggle/working/gpt2_mid_gsmplus_inference.csv\")\n\n# # Initialize metrics\n# scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n# results = []\n\n# def calculate_exact_match(pred, ref):\n#     return int(pred.strip() == ref.strip())\n\n# def calculate_token_f1(pred, ref):\n#     pred_tokens = pred.split()\n#     ref_tokens = ref.split()\n    \n#     common_tokens = set(pred_tokens) & set(ref_tokens)\n#     if len(pred_tokens) == 0 or len(ref_tokens) == 0:\n#         return 0.0\n    \n#     precision = len(common_tokens) / len(pred_tokens)\n#     recall = len(common_tokens) / len(ref_tokens)\n    \n#     if (precision + recall) == 0:\n#         return 0.0\n#     return 2 * (precision * recall) / (precision + recall)\n\n# # Calculate metrics for each row\n# for _, row in df.iterrows():\n#     rouge_scores = scorer.score(row['reference_answer'], row['model_answer'])\n#     results.append({\n#         'rouge1': rouge_scores['rouge1'].fmeasure,\n#         'rouge2': rouge_scores['rouge2'].fmeasure,\n#         'rougeL': rouge_scores['rougeL'].fmeasure,\n#         'exact_match': calculate_exact_match(row['model_answer'], row['reference_answer']),\n#         'token_f1': calculate_token_f1(row['model_answer'], row['reference_answer'])\n#     })\n\n# # Add metrics to original dataframe\n# metrics_df = pd.DataFrame(results)\n# final_df = pd.concat([df, metrics_df], axis=1)\n\n# # Save results with metrics\n# final_df.to_csv(\"/kaggle/working/gpt2_mid_gsmplus_inference_with_metrics.csv\", index=False)\n\n# # Calculate and print aggregate metrics\n# aggregate_metrics = {\n#     'ROUGE-1': np.mean(metrics_df['rouge1']),\n#     'ROUGE-2': np.mean(metrics_df['rouge2']),\n#     'ROUGE-L': np.mean(metrics_df['rougeL']),\n#     'Exact Match': np.mean(metrics_df['exact_match']),\n#     'Token F1': np.mean(metrics_df['token_f1'])\n# }\n\n# print(\"\\nAggregate Metrics:\")\n# for metric, value in aggregate_metrics.items():\n#     print(f\"{metric}: {value:.4f}\")\n\n# # Save aggregate metrics\n# pd.DataFrame([aggregate_metrics]).to_csv(\"/kaggle/working/gpt-2mid_aggregate_metrics.csv\", index=False)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}